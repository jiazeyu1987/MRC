"""
Document Service - Core document management operations and lifecycle management.

This service provides business logic for document operations including CRUD,
status management, and synchronization with RAGFlow.
"""

from datetime import datetime
from typing import List, Dict, Optional, Any
from sqlalchemy import and_, or_, desc, asc
from app import db
from app.models.document import Document
from app.models.document_chunk import DocumentChunk
from app.models.processing_log import ProcessingLog
from app.models.knowledge_base import KnowledgeBase
from app.services.ragflow_service import RAGFlowService
from app.services.cache_service import CacheService
import logging

logger = logging.getLogger(__name__)

class DocumentService:
    def __init__(self):
        from app.services.ragflow_service import get_ragflow_service

        ragflow_service = get_ragflow_service()
        if not ragflow_service:
            from app.services.ragflow_service import RAGFlowConfigError
            raise RAGFlowConfigError("RAGFlow service not available")
        self.ragflow_service = ragflow_service
        self.cache_service = CacheService()

    def get_documents(self, knowledge_base_id: str, filters: Dict = None) -> List[Dict]:
        """
        Retrieve documents for a knowledge base from RAGFlow with optional filtering.

        Args:
            knowledge_base_id: ID of the knowledge base
            filters: Optional filters (search, status, file_type, sort_by, sort_order, page, limit)

        Returns:
            List of document dictionaries from RAGFlow
        """
        try:
            # Get knowledge base to find RAGFlow dataset ID
            knowledge_base = KnowledgeBase.query.get(knowledge_base_id)
            if not knowledge_base or not knowledge_base.ragflow_dataset_id:
                logger.warning(f"Knowledge base {knowledge_base_id} not found or no RAGFlow dataset ID")
                return []

            # Prepare RAGFlow parameters
            ragflow_status = None
            ragflow_page = filters.get('page', 1) if filters else 1
            ragflow_size = filters.get('limit', 20) if filters else 20

            # Status filter (RAGFlow uses parsing_status)
            if filters and filters.get('status'):
                status_map = {
                    'uploaded': 'waiting',
                    'processing': 'parsing',
                    'completed': 'completed',
                    'failed': 'failed'
                }
                ragflow_status = status_map.get(filters['status'], filters['status'])

            # Get documents from RAGFlow with filters
            ragflow_result = self.ragflow_service.get_dataset_documents(
                dataset_id=knowledge_base.ragflow_dataset_id,
                status=ragflow_status,
                page=ragflow_page,
                size=ragflow_size
            )

            # Handle the new return format (dictionary with 'documents' key) or old format (list)
            if isinstance(ragflow_result, dict) and 'documents' in ragflow_result:
                ragflow_documents = ragflow_result['documents']
                # Extract total count for pagination if available
                total_count = ragflow_result.get('total_count', len(ragflow_documents))
            elif isinstance(ragflow_result, list):
                ragflow_documents = ragflow_result
                total_count = len(ragflow_documents)
            else:
                ragflow_documents = []
                total_count = 0

            # Apply additional client-side filters if needed
            if filters:
                filtered_documents = ragflow_documents

                # Search filter (client-side since RAGFlow API might not support it)
                if filters.get('search'):
                    search_term = filters['search'].lower()
                    filtered_documents = [
                        doc for doc in filtered_documents
                        if search_term in doc.get('name', '').lower()
                    ]

                # File type filter
                if filters.get('file_type'):
                    filtered_documents = [
                        doc for doc in filtered_documents
                        if doc.get('suffix', '').lower() == filters['file_type'].lower()
                    ]

                ragflow_documents = filtered_documents

            # Transform RAGFlow documents to frontend-compatible format
            transformed_documents = []
            for doc in ragflow_documents:
                # Map RAGFlow status to frontend status
                run_status = doc.get('run', 'UNSTART')
                upload_status = 'uploaded'
                processing_status = 'pending'

                if run_status == 'DONE':
                    processing_status = 'completed'
                elif run_status == 'RUNNING':
                    processing_status = 'processing'
                elif run_status == 'FAIL':
                    processing_status = 'failed'

                transformed_doc = {
                    'id': doc.get('id', ''),
                    'knowledge_base_id': knowledge_base_id,
                    'ragflow_document_id': doc.get('id', ''),
                    'filename': doc.get('name', ''),
                    'original_filename': doc.get('name', ''),
                    'file_size': doc.get('size', 0),
                    'file_type': doc.get('suffix', '').lower(),
                    'mime_type': "application/" + doc.get('suffix', '').lower(),
                    'upload_status': upload_status,
                    'processing_status': processing_status,
                    'chunk_count': doc.get('chunk_count', 0),
                    'ragflow_metadata': doc,
                    'created_at': self._convert_timestamp(doc.get('create_time')),
                    'updated_at': self._convert_timestamp(doc.get('update_time')),
                }
                transformed_documents.append(transformed_doc)

            logger.info(f"Retrieved {len(transformed_documents)} documents from RAGFlow for knowledge base {knowledge_base_id}")

            # Return both documents and total count for proper pagination
            return {
                'documents': transformed_documents,
                'total_count': total_count
            }

        except Exception as e:
            logger.error(f"Error retrieving documents from RAGFlow for knowledge base {knowledge_base_id}: {e}")
            raise

    def get_document(self, document_id: str) -> Optional[Document]:
        """
        Get a single document by ID.

        Args:
            document_id: ID of the document

        Returns:
            Document object or None if not found
        """
        try:
            document = Document.query.get(document_id)
            if document:
                logger.debug(f"Retrieved document {document_id}")
            else:
                logger.warning(f"Document {document_id} not found")
            return document

        except Exception as e:
            logger.error(f"Error retrieving document {document_id}: {e}")
            raise

    def create_document(self, knowledge_base_id: str, filename: str, original_filename: str,
                       file_size: int, file_type: str, mime_type: str, ragflow_document_id: str = None) -> Document:
        """
        Create a new document record.

        Args:
            knowledge_base_id: ID of the knowledge base
            filename: Stored filename (may be different from original)
            original_filename: Original filename from upload
            file_size: File size in bytes
            file_type: File extension/type
            mime_type: MIME type
            ragflow_document_id: Optional RAGFlow document ID

        Returns:
            Created Document object
        """
        try:
            document = Document(
                knowledge_base_id=knowledge_base_id,
                filename=filename,
                original_filename=original_filename,
                file_size=file_size,
                file_type=file_type,
                mime_type=mime_type,
                ragflow_document_id=ragflow_document_id,
                upload_status='uploaded' if ragflow_document_id else 'uploading',
                processing_status='pending'
            )

            db.session.add(document)
            db.session.commit()

            # Start processing log
            ProcessingLog.start_step(document.id, 'upload', 'Document created successfully')

            logger.info(f"Created document {document.id} in knowledge base {knowledge_base_id}")
            return document

        except Exception as e:
            logger.error(f"Error creating document in knowledge base {knowledge_base_id}: {e}")
            db.session.rollback()
            raise

    def update_document_status(self, document_id: str, upload_status: str = None,
                             processing_status: str = None, error_message: str = None) -> bool:
        """
        Update document status and optionally log the change.

        Args:
            document_id: ID of the document
            upload_status: New upload status
            processing_status: New processing status
            error_message: Optional error message

        Returns:
            True if successful, False otherwise
        """
        try:
            document = Document.query.get(document_id)
            if not document:
                logger.warning(f"Document {document_id} not found for status update")
                return False

            old_status = f"{document.upload_status}/{document.processing_status}"
            document.update_status(upload_status, processing_status, error_message)
            new_status = f"{document.upload_status}/{document.processing_status}"

            # Log significant status changes
            if old_status != new_status:
                logger.info(f"Document {document_id} status changed: {old_status} -> {new_status}")

                # Clear cache for this knowledge base
                self.cache_service.delete_pattern(f"documents:{document.knowledge_base_id}:*")

            return True

        except Exception as e:
            logger.error(f"Error updating document {document_id} status: {e}")
            return False

    def delete_document(self, document_id: str, delete_from_ragflow: bool = True) -> bool:
        """
        Delete a document from system and optionally from RAGFlow.

        Args:
            document_id: ID of the document to delete
            delete_from_ragflow: Whether to also delete from RAGFlow

        Returns:
            True if successful, False otherwise
        """
        try:
            document = Document.query.get(document_id)
            if not document:
                logger.warning(f"Document {document_id} not found for deletion")
                return False

            kb_id = document.knowledge_base_id

            # Delete from RAGFlow first if requested and has RAGFlow ID
            if delete_from_ragflow and document.ragflow_document_id:
                try:
                    self.ragflow_service.delete_document(document.ragflow_document_id)
                    logger.info(f"Deleted document {document_id} from RAGFlow")
                except Exception as e:
                    logger.error(f"Failed to delete document {document_id} from RAGFlow: {e}")
                    # Continue with local deletion even if RAGFlow deletion fails

            # Delete from local database (cascades will handle chunks and logs)
            db.session.delete(document)
            db.session.commit()

            # Clear cache
            self.cache_service.delete_pattern(f"documents:{kb_id}:*")

            logger.info(f"Deleted document {document_id} from knowledge base {kb_id}")
            return True

        except Exception as e:
            logger.error(f"Error deleting document {document_id}: {e}")
            db.session.rollback()
            return False

    def sync_with_ragflow(self, knowledge_base_id: str) -> Dict[str, Any]:
        """
        Synchronize documents with RAGFlow dataset.

        Args:
            knowledge_base_id: ID of the knowledge base

        Returns:
            Sync result statistics
        """
        try:
            knowledge_base = KnowledgeBase.query.get(knowledge_base_id)
            if not knowledge_base:
                raise ValueError(f"Knowledge base {knowledge_base_id} not found")

            # Get documents from RAGFlow
            ragflow_docs = self.ragflow_service.get_dataset_documents(knowledge_base.ragflow_dataset_id)

            # Get local documents
            local_docs = Document.query.filter_by(knowledge_base_id=knowledge_base_id).all()

            # Map local documents by RAGFlow ID
            local_doc_map = {doc.ragflow_document_id: doc for doc in local_docs if doc.ragflow_document_id}

            sync_result = {
                'total_ragflow': len(ragflow_docs),
                'total_local': len(local_docs),
                'added': 0,
                'updated': 0,
                'deleted': 0,
                'errors': []
            }

            # Find documents to add or update
            for rf_doc in ragflow_docs:
                rf_doc_id = rf_doc.get('id')

                if rf_doc_id in local_doc_map:
                    # Update existing document
                    local_doc = local_doc_map[rf_doc_id]
                    if self._update_document_from_ragflow(local_doc, rf_doc):
                        sync_result['updated'] += 1
                else:
                    # Add new document from RAGFlow
                    try:
                        self._create_document_from_ragflow(knowledge_base_id, rf_doc)
                        sync_result['added'] += 1
                    except Exception as e:
                        sync_result['errors'].append(f"Failed to add document {rf_doc_id}: {e}")

            # Find documents to delete (local docs not in RAGFlow)
            rf_doc_ids = {doc.get('id') for doc in ragflow_docs}
            for local_doc in local_docs:
                if local_doc.ragflow_document_id and local_doc.ragflow_document_id not in rf_doc_ids:
                    try:
                        self.delete_document(local_doc.id, delete_from_ragflow=False)
                        sync_result['deleted'] += 1
                    except Exception as e:
                        sync_result['errors'].append(f"Failed to delete local document {local_doc.id}: {e}")

            # Clear cache
            self.cache_service.delete_pattern(f"documents:{knowledge_base_id}:*")

            logger.info(f"RAGFlow sync completed for knowledge base {knowledge_base_id}: {sync_result}")
            return sync_result

        except Exception as e:
            logger.error(f"Error syncing RAGFlow for knowledge base {knowledge_base_id}: {e}")
            raise

    def get_document_statistics(self, knowledge_base_id: str) -> Dict[str, Any]:
        """
        Get statistics for documents in a knowledge base.

        Args:
            knowledge_base_id: ID of the knowledge base

        Returns:
            Dictionary with document statistics
        """
        try:
            # Use cache first
            cache_key = f"documents:{knowledge_base_id}:statistics"
            cached_stats = self.cache_service.get(cache_key)
            if cached_stats:
                return cached_stats

            # Query database for statistics
            total_docs = Document.query.filter_by(knowledge_base_id=knowledge_base_id).count()

            # Status breakdown
            status_stats = db.session.query(
                Document.processing_status,
                db.func.count(Document.id).label('count')
            ).filter_by(knowledge_base_id=knowledge_base_id).group_by(Document.processing_status).all()

            status_breakdown = {status: count for status, count in status_stats}

            # File type breakdown
            type_stats = db.session.query(
                Document.file_type,
                db.func.count(Document.id).label('count')
            ).filter_by(knowledge_base_id=knowledge_base_id).group_by(Document.file_type).all()

            type_breakdown = {file_type: count for file_type, count in type_stats}

            # Total file size
            total_size = db.session.query(
                db.func.sum(Document.file_size)
            ).filter_by(knowledge_base_id=knowledge_base_id).scalar() or 0

            # Total chunks
            total_chunks = db.session.query(
                db.func.count(DocumentChunk.id)
            ).join(Document).filter(Document.knowledge_base_id == knowledge_base_id).scalar() or 0

            statistics = {
                'total_documents': total_docs,
                'total_file_size_bytes': total_size,
                'total_file_size_mb': round(total_size / (1024 * 1024), 2),
                'total_chunks': total_chunks,
                'status_breakdown': status_breakdown,
                'file_type_breakdown': type_breakdown,
                'updated_at': datetime.utcnow().isoformat()
            }

            # Cache for 5 minutes
            self.cache_service.set(cache_key, statistics, timeout=300)

            return statistics

        except Exception as e:
            logger.error(f"Error getting document statistics for knowledge base {knowledge_base_id}: {e}")
            raise

    def _update_document_from_ragflow(self, document: Document, ragflow_doc: Dict) -> bool:
        """Update local document from RAGFlow document data."""
        try:
            # Update RAGFlow metadata
            document.ragflow_metadata = ragflow_doc

            # Update processing status based on RAGFlow status
            rf_status = ragflow_doc.get('status', 'unknown')
            if rf_status == 'available':
                if document.processing_status != 'completed':
                    document.update_status(processing_status='completed')
            elif rf_status == 'error':
                error_msg = ragflow_doc.get('error', 'Unknown RAGFlow error')
                document.update_status(processing_status='failed', error_message=error_msg)

            db.session.commit()
            return True

        except Exception as e:
            logger.error(f"Error updating document {document.id} from RAGFlow: {e}")
            return False

    def _create_document_from_ragflow(self, knowledge_base_id: str, ragflow_doc: Dict) -> Document:
        """Create local document from RAGFlow document data."""
        try:
            rf_doc_id = ragflow_doc.get('id')
            rf_name = ragflow_doc.get('name', f'ragflow_doc_{rf_doc_id}')
            rf_size = ragflow_doc.get('size', 0)

            # Extract file type from name
            file_type = rf_name.split('.')[-1].lower() if '.' in rf_name else 'unknown'

            document = self.create_document(
                knowledge_base_id=knowledge_base_id,
                filename=rf_name,
                original_filename=rf_name,
                file_size=rf_size,
                file_type=file_type,
                mime_type=f'application/{file_type}',
                ragflow_document_id=rf_doc_id
            )

            # Update from RAGFlow data
            self._update_document_from_ragflow(document, ragflow_doc)

            return document

        except Exception as e:
            logger.error(f"Error creating document from RAGFlow data: {e}")
            raise

    def _convert_timestamp(self, timestamp: Optional[int]) -> str:
        """Convert RAGFlow timestamp to ISO datetime string"""
        if not timestamp:
            return datetime.utcnow().isoformat()
        try:
            # RAGFlow uses Unix timestamp in milliseconds
            return datetime.fromtimestamp(timestamp / 1000).isoformat()
        except (ValueError, OSError):
            return datetime.utcnow().isoformat()